{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/Y1CTNy9.jpeg\">\n\n# <b><span style='color:#F1A424'>|</span> DAIGT: <span style='color:#F1A424'>Text Classification</span><span style='color:#ABABAB'> [Inference]</span></b>\n\n***\n\n\n### <b><span style='color:#F1A424'>Table of Contents</span></b> <a class='anchor' id='top'></a>\n<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<li> <a href=\"#introduction\">Introduction</a></li>\n<li> <a href=\"#install_libraries\">Install libraries</a></li>\n<li><a href=\"#import_libraries\">Import Libraries</a></li>\n<li><a href=\"#configuration\">Configuration</a></li>\n<li><a href=\"#utils\">Utils</a></li>\n<li><a href=\"#load_data\">Load Data</a></li>\n<li><a href=\"#dataset\">Dataset</a></li>\n<li><a href=\"#model\">Model</a></li>\n<li><a href=\"#inference_function\">Inference Function</a></li>\n<li><a href=\"#inference\">Inference</a></li>\n<li><a href=\"#submission\">Save Submission</a></li>\n</div>\n\n\n# <b><span style='color:#F1A424'>|</span> Introduction</b><a class='anchor' id='introduction'></a> [↑](#top)\n\n***\n\n### <b><span style='color:#F1A424'>Useful References</span></b>\n\n- [DAIGT | Deberta Text Classification [Train]](https://www.kaggle.com/alejopaullier/daigt-deberta-text-classification-train)\n- [Y.Nakama | FB3 / Deberta-v3-base baseline [train]](https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)\n- [Y.Nakama | FB3 / Deberta-v3-base baseline [inference]](https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-inference/notebook)\n- [Deberta v3 Hugging Face](https://huggingface.co/microsoft/deberta-v3-base)\n- [Deberta paper](https://arxiv.org/abs/2006.03654)","metadata":{"papermill":{"duration":0.012455,"end_time":"2022-08-31T07:01:57.321119","exception":false,"start_time":"2022-08-31T07:01:57.308664","status":"completed"},"tags":[],"id":"izcnVGzfFVW5"}},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Install Libraries</b><a class='anchor' id='install_libraries'></a> [↑](#top) \n\n***\n\nInstall the necessary libraries.","metadata":{}},{"cell_type":"code","source":"# !pip uninstall transformers -y\n# !python -m pip install --no-index --find-links=/kaggle/input/benetech-pip transformers","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2023-11-01T00:19:49.748464Z","iopub.execute_input":"2023-11-01T00:19:49.748814Z","iopub.status.idle":"2023-11-01T00:19:49.756449Z","shell.execute_reply.started":"2023-11-01T00:19:49.748785Z","shell.execute_reply":"2023-11-01T00:19:49.755558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Import Libraries</b><a class='anchor' id='import_libraries'></a> [↑](#top)\n\n***\n\nImport all the required libraries for this notebook.","metadata":{"id":"QJn3zFXNFVW7"}},{"cell_type":"code","source":"import ast\nimport copy\nimport gc\nimport itertools\nimport joblib\nimport json\nimport math\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport random\nimport re\nimport scipy as sp\nimport string\nimport sys\nimport time\nimport warnings\n\n\nfrom sklearn.metrics import mean_squared_error\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.auto import tqdm\n\n# ======= OPTIONS =========\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Current device is: {device}\")\nwarnings.filterwarnings(\"ignore\")","metadata":{"_kg_hide-input":true,"id":"oHbaEW3yFVW7","executionInfo":{"status":"ok","timestamp":1689954823487,"user_tz":180,"elapsed":2443,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"outputId":"d97e382c-6902-48ce-d2cd-23e96fc7caa0","execution":{"iopub.status.busy":"2023-11-01T04:52:06.832007Z","iopub.execute_input":"2023-11-01T04:52:06.832281Z","iopub.status.idle":"2023-11-01T04:52:11.295486Z","shell.execute_reply.started":"2023-11-01T04:52:06.832257Z","shell.execute_reply":"2023-11-01T04:52:11.294491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Tokenizers and transformers</span></b>","metadata":{"id":"c7Ft7SnDFVW8"}},{"cell_type":"code","source":"import tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")","metadata":{"_kg_hide-input":true,"scrolled":true,"_kg_hide-output":true,"id":"y-J2UYpGFVW8","executionInfo":{"status":"ok","timestamp":1689954838880,"user_tz":180,"elapsed":15396,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"outputId":"4f92ee93-b6b6-4517-e2fa-e11f2c8890da","execution":{"iopub.status.busy":"2023-11-01T04:52:16.819781Z","iopub.execute_input":"2023-11-01T04:52:16.820604Z","iopub.status.idle":"2023-11-01T04:52:25.181962Z","shell.execute_reply.started":"2023-11-01T04:52:16.820569Z","shell.execute_reply":"2023-11-01T04:52:25.180934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Configuration</b><a class='anchor' id='configuration'></a> [↑](#top)\n\n***\n\nCentral repository for this notebook's hyperparameters.","metadata":{"papermill":{"duration":0.006569,"end_time":"2022-08-31T07:01:57.395826","exception":false,"start_time":"2022-08-31T07:01:57.389257","status":"completed"},"tags":[],"id":"usLvJDiuFVW9"}},{"cell_type":"code","source":"class config:\n    BATCH_SIZE_TEST = 16\n    DEBUG = False\n    GRADIENT_CHECKPOINTING = True\n    MAX_LEN = 512\n    MODEL = \"microsoft/deberta-v3-base\"\n    NUM_WORKERS = 0 \n    PRINT_FREQ = 20\n    SEED = 42\n\n\nclass paths:\n    MODEL_PATH = \"/kaggle/input/daigt-deberta-text-classification-train/output\"\n    BEST_MODEL_PATH = \"/kaggle/input/daigt-deberta-text-classification-train/output/microsoft_deberta-v3-base_fold_0_best.pth\"\n    TEST_ESSAYS = \"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\"\n    SUBMISSION_CSV = \"/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\"","metadata":{"papermill":{"duration":0.015868,"end_time":"2022-08-31T07:01:57.417077","exception":false,"start_time":"2022-08-31T07:01:57.401209","status":"completed"},"tags":[],"id":"46hdYM64FVW9","executionInfo":{"status":"ok","timestamp":1689955017944,"user_tz":180,"elapsed":1,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"execution":{"iopub.status.busy":"2023-11-01T04:52:41.860782Z","iopub.execute_input":"2023-11-01T04:52:41.861863Z","iopub.status.idle":"2023-11-01T04:52:41.867272Z","shell.execute_reply.started":"2023-11-01T04:52:41.861820Z","shell.execute_reply":"2023-11-01T04:52:41.866380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Utils</b><a class='anchor' id='utils'></a> [↑](#top)\n\n***\n\nUtility functions used throughout the notebook.","metadata":{"papermill":{"duration":0.007998,"end_time":"2022-08-31T07:03:04.079768","exception":false,"start_time":"2022-08-31T07:03:04.07177","status":"completed"},"tags":[],"id":"7scCSMytFVW9"}},{"cell_type":"code","source":"def get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores\n\n\ndef seed_everything(seed=20):\n    \"\"\"Seed everything to ensure reproducibility\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef sep():\n    print(\"-\"*100)\n    \n    \ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))  \n    \nseed_everything(seed=config.SEED)","metadata":{"papermill":{"duration":0.024132,"end_time":"2022-08-31T07:03:04.111108","exception":false,"start_time":"2022-08-31T07:03:04.086976","status":"completed"},"tags":[],"id":"zANL8ukPFVW9","executionInfo":{"status":"ok","timestamp":1689954887585,"user_tz":180,"elapsed":3,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"execution":{"iopub.status.busy":"2023-11-01T04:52:43.566951Z","iopub.execute_input":"2023-11-01T04:52:43.567312Z","iopub.status.idle":"2023-11-01T04:52:43.580231Z","shell.execute_reply.started":"2023-11-01T04:52:43.567283Z","shell.execute_reply":"2023-11-01T04:52:43.579282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Load Data</b><a class='anchor' id='load_data'></a> [↑](#top)\n\n***\n\nLoad data.","metadata":{"papermill":{"duration":0.012589,"end_time":"2022-08-31T07:03:04.13341","exception":false,"start_time":"2022-08-31T07:03:04.120821","status":"completed"},"tags":[],"id":"1KzRADqxFVW9"}},{"cell_type":"code","source":"test_df = pd.read_csv(paths.TEST_ESSAYS, sep=',')\nprint(f\"Test summaries dataframe has shape: {test_df.shape}\"), sep()\ndisplay(test_df.head())","metadata":{"papermill":{"duration":0.242687,"end_time":"2022-08-31T07:03:04.383434","exception":false,"start_time":"2022-08-31T07:03:04.140747","status":"completed"},"tags":[],"id":"Ex8qtc0nFVW-","executionInfo":{"status":"ok","timestamp":1689954892806,"user_tz":180,"elapsed":495,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"outputId":"26ab910f-ee3a-4a2d-9181-e3840821242b","execution":{"iopub.status.busy":"2023-11-01T04:52:44.602212Z","iopub.execute_input":"2023-11-01T04:52:44.602615Z","iopub.status.idle":"2023-11-01T04:52:44.630341Z","shell.execute_reply.started":"2023-11-01T04:52:44.602583Z","shell.execute_reply":"2023-11-01T04:52:44.629488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer <a class='anchor' id='tokenizer'> [↑](#top)","metadata":{"papermill":{"duration":0.007561,"end_time":"2022-08-31T07:03:04.604916","exception":false,"start_time":"2022-08-31T07:03:04.597355","status":"completed"},"tags":[],"id":"b1z0TQdhFVW-"}},{"cell_type":"code","source":"# === Load tokenizer ===\ntokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH + \"/tokenizer\")\n# === Add special tokens ===\nvocabulary = tokenizer.get_vocab()\ntotal_tokens = len(vocabulary)\nprint(\"Total number of tokens in the tokenizer:\", total_tokens)\nprint(tokenizer)","metadata":{"papermill":{"duration":7.351568,"end_time":"2022-08-31T07:03:11.964298","exception":false,"start_time":"2022-08-31T07:03:04.61273","status":"completed"},"tags":[],"id":"0JgNbcB3FVW-","executionInfo":{"status":"ok","timestamp":1689954898275,"user_tz":180,"elapsed":1549,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"outputId":"cd9bc172-b06c-40b1-edb4-afc6f0d68e7f","execution":{"iopub.status.busy":"2023-11-01T04:52:46.072539Z","iopub.execute_input":"2023-11-01T04:52:46.072920Z","iopub.status.idle":"2023-11-01T04:52:46.573703Z","shell.execute_reply.started":"2023-11-01T04:52:46.072888Z","shell.execute_reply":"2023-11-01T04:52:46.572670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Dataset</b><a class='anchor' id='dataset'></a> [↑](#top)\n\n***\n\n    \nWe need to get the `max_len` from our `tokenizer`. We create a `tqdm` iterator and for each text we extract the tokenized length. Then we get the maximum value and we add 3 for the special tokens `CLS`, `SEP`, `SEP`.\n\n- [Hugging Face Padding and Truncation](https://huggingface.co/docs/transformers/pad_truncation): check truncation to `max_length` or `True` (batch max length).","metadata":{"papermill":{"duration":0.008127,"end_time":"2022-08-31T07:03:11.985369","exception":false,"start_time":"2022-08-31T07:03:11.977242","status":"completed"},"tags":[],"id":"5wkNuxLpFVW-"}},{"cell_type":"code","source":"lengths = []\ntqdm_loader = tqdm(test_df['text'].fillna(\"\").values, total=len(test_df))\nfor text in tqdm_loader:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\n    \n_ = plt.hist(lengths, bins=25)","metadata":{"papermill":{"duration":5.893032,"end_time":"2022-08-31T07:03:17.886504","exception":false,"start_time":"2022-08-31T07:03:11.993472","status":"completed"},"tags":[],"id":"vyEylZmtFVW-","executionInfo":{"status":"ok","timestamp":1689954905446,"user_tz":180,"elapsed":2736,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"outputId":"90e85de7-5775-4aeb-e83b-a201cce38dec","execution":{"iopub.status.busy":"2023-11-01T04:52:47.002980Z","iopub.execute_input":"2023-11-01T04:52:47.003683Z","iopub.status.idle":"2023-11-01T04:52:47.276443Z","shell.execute_reply.started":"2023-11-01T04:52:47.003650Z","shell.execute_reply":"2023-11-01T04:52:47.275435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text, tokenizer):\n    \"\"\"\n    This function tokenizes the input text with the configured padding and truncation. Then,\n    returns the input dictionary, which contains the following keys: \"input_ids\",\n    \"token_type_ids\" and \"attention_mask\". Each value is a torch.tensor.\n    :param cfg: configuration class with a TOKENIZER attribute.\n    :param text: a numpy array where each value is a text as string.\n    :return inputs: python dictionary where values are torch tensors.\n    \"\"\"\n    inputs = tokenizer.encode_plus(\n        text,\n        return_tensors=None,\n        add_special_tokens=True,\n        max_length=cfg.MAX_LEN,\n        padding='max_length', # TODO: check padding to max sequence in batch\n        truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long) # TODO: check dtypes\n    return inputs\n\n\ndef collate(inputs):\n    \"\"\"\n    It truncates the inputs to the maximum sequence length in the batch.\n    \"\"\"\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max()) # Get batch's max sequence length\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, cfg, df, tokenizer):\n        self.cfg = cfg\n        self.texts = df['text'].values\n        self.tokenizer = tokenizer\n        self.ids = df['id'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        output = {}\n        output[\"inputs\"] = prepare_input(self.cfg, self.texts[item], self.tokenizer)\n        output[\"ids\"] = self.ids[item]\n        return output","metadata":{"papermill":{"duration":0.020447,"end_time":"2022-08-31T07:03:17.916566","exception":false,"start_time":"2022-08-31T07:03:17.896119","status":"completed"},"tags":[],"id":"litBUfI6FVW-","executionInfo":{"status":"ok","timestamp":1689954907734,"user_tz":180,"elapsed":5,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"execution":{"iopub.status.busy":"2023-11-01T04:52:47.822936Z","iopub.execute_input":"2023-11-01T04:52:47.823614Z","iopub.status.idle":"2023-11-01T04:52:47.833437Z","shell.execute_reply.started":"2023-11-01T04:52:47.823582Z","shell.execute_reply":"2023-11-01T04:52:47.832592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One sample from the dataset should look as following:\n```python\n{\n\t'inputs': {\n\t\t'input_ids': tensor([1, 279, 883, ..., 0, 0]),\n\t\t'token_type_ids': tensor([0, 0, 0, ..., 0, 0]),\n\t\t'attention_mask': tensor([1, 1, 1, ..., 0, 0])\n\t},\n\t'label': tensor([0.2057, 0.3805]),\n\t'ids': '000e8c3c7ddb'\n}\n```\nYou can check it by running the cell below.","metadata":{"id":"bMpipBIOFVW_"}},{"cell_type":"code","source":"if config.DEBUG:\n    # ======== DATASETS ==========\n    test_dataset = CustomDataset(config, test_df, tokenizer)\n\n    # ======== DATALOADERS ==========\n    test_loader = DataLoader(test_dataset,\n                             batch_size=config.BATCH_SIZE_TEST,\n                             shuffle=False,\n                             num_workers=config.NUM_WORKERS,\n                             pin_memory=True, drop_last=False)\n\n    # === Let's check one sample ===\n    sample = test_dataset[0]\n    print(f\"Encoding keys: {sample.keys()} \\n\")\n    print(sample)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"2osaa0_9FVW_","executionInfo":{"status":"ok","timestamp":1689954909869,"user_tz":180,"elapsed":4,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"execution":{"iopub.status.busy":"2023-11-01T04:52:48.774518Z","iopub.execute_input":"2023-11-01T04:52:48.775177Z","iopub.status.idle":"2023-11-01T04:52:48.780522Z","shell.execute_reply.started":"2023-11-01T04:52:48.775136Z","shell.execute_reply":"2023-11-01T04:52:48.779489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Model</b><a class='anchor' id='model'></a> [↑](#top)\n\n***","metadata":{"papermill":{"duration":0.008073,"end_time":"2022-08-31T07:03:17.933189","exception":false,"start_time":"2022-08-31T07:03:17.925116","status":"completed"},"tags":[],"id":"7epn7vaTFVW_"}},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.dropout = 0.2\n        # Load config by inferencing it from the model name.\n        if config_path is None: \n            self.config = AutoConfig.from_pretrained(cfg.MODEL, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n        # Load config from a file.\n        else:\n            self.config = torch.load(config_path)\n        \n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.MODEL, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        \n        if self.cfg.GRADIENT_CHECKPOINTING:\n            self.model.gradient_checkpointing_enable()\n          \n        # Add MeanPooling and Linear head at the end to transform the Model into a RegressionModel\n        self.pool = MeanPooling()\n        self.head = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(self.dropout),\n            nn.Linear(64, 16),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(self.dropout),\n            nn.Linear(16, 1)\n        )\n#         self._init_weights(self.head)\n        \n    def _init_weights(self, module):\n        \"\"\"\n        This method initializes weights for different types of layers. The type of layers \n        supported are nn.Linear, nn.Embedding and nn.LayerNorm.\n        \"\"\"\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        \"\"\"\n        This method makes a forward pass through the model, get the last hidden state (embedding)\n        and pass it through the MeanPooling layer.\n        \"\"\"\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        return feature\n\n    def forward(self, inputs):\n        \"\"\"\n        This method makes a forward pass through the model, the MeanPooling layer and finally\n        then through the Linear layer to get a regression value.\n        \"\"\"\n        feature = self.feature(inputs)\n        output = self.head(feature)\n        return output","metadata":{"papermill":{"duration":0.033105,"end_time":"2022-08-31T07:03:17.97447","exception":false,"start_time":"2022-08-31T07:03:17.941365","status":"completed"},"tags":[],"id":"bqvvjU6mFVW_","executionInfo":{"status":"ok","timestamp":1689954910341,"user_tz":180,"elapsed":2,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T04:52:49.580290Z","iopub.execute_input":"2023-11-01T04:52:49.581158Z","iopub.status.idle":"2023-11-01T04:52:49.598290Z","shell.execute_reply.started":"2023-11-01T04:52:49.581111Z","shell.execute_reply":"2023-11-01T04:52:49.597303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Inference Function</b><a class='anchor' id='inference_function'></a> [↑](#top)\n\n***","metadata":{"papermill":{"duration":0.008452,"end_time":"2022-08-31T07:03:18.041557","exception":false,"start_time":"2022-08-31T07:03:18.033105","status":"completed"},"tags":[],"id":"xwJe_2uxFVW_"}},{"cell_type":"code","source":"def inference_fn(config, test_df, tokenizer, device):\n    # ======== DATASETS ==========\n    test_dataset = CustomDataset(config, test_df, tokenizer)\n\n    # ======== DATALOADERS ==========\n    test_loader = DataLoader(test_dataset,\n                             batch_size=config.BATCH_SIZE_TEST,\n                             shuffle=False,\n                             num_workers=0,\n                             pin_memory=True, drop_last=False)\n    \n    # ======== MODEL ==========\n    model = CustomModel(config, config_path=paths.MODEL_PATH + \"/config.pth\", pretrained=False)\n    state = torch.load(paths.BEST_MODEL_PATH)\n    model.load_state_dict(state)\n    model.to(device)\n    model.eval() # set model in evaluation mode\n    prediction_dict = {}\n    preds = []\n    with tqdm(test_loader, unit=\"test_batch\", desc='Inference') as tqdm_test_loader:\n        for step, batch in enumerate(tqdm_test_loader):\n            inputs = batch.pop(\"inputs\")\n            ids = batch.pop(\"ids\")\n            inputs = collate(inputs) # collate inputs\n            for k, v in inputs.items():\n                inputs[k] = v.to(device) # send inputs to device\n            with torch.no_grad():\n                y_preds = model(inputs) # forward propagation pass\n            preds.append(y_preds.to('cpu').numpy()) # save predictions\n\n    prediction_dict[\"predictions\"] = np.concatenate(preds) # np.array() of shape (fold_size, target_cols)\n    prediction_dict[\"ids\"] = ids\n    return prediction_dict","metadata":{"papermill":{"duration":0.030759,"end_time":"2022-08-31T07:03:18.08056","exception":false,"start_time":"2022-08-31T07:03:18.049801","status":"completed"},"tags":[],"id":"ysjawRSEFVW_","executionInfo":{"status":"ok","timestamp":1689954912993,"user_tz":180,"elapsed":5,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"execution":{"iopub.status.busy":"2023-11-01T04:52:50.512554Z","iopub.execute_input":"2023-11-01T04:52:50.513399Z","iopub.status.idle":"2023-11-01T04:52:50.522613Z","shell.execute_reply.started":"2023-11-01T04:52:50.513370Z","shell.execute_reply":"2023-11-01T04:52:50.521524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Inference</b><a class='anchor' id='inference'></a> [↑](#top)\n\n***","metadata":{"id":"Ykf7zdECFVXA"}},{"cell_type":"code","source":"predictions = inference_fn(config, test_df, tokenizer, device)","metadata":{"papermill":{"duration":11935.46951,"end_time":"2022-08-31T10:22:13.621316","exception":false,"start_time":"2022-08-31T07:03:18.151806","status":"completed"},"tags":[],"id":"wYwfXGZxFVXA","executionInfo":{"status":"ok","timestamp":1689963166226,"user_tz":180,"elapsed":8139690,"user":{"displayName":"Alejo Paullier","userId":"10171290888380131520"}},"outputId":"0245d2b0-8080-4d44-d96d-63f96ec12a48","execution":{"iopub.status.busy":"2023-11-01T04:52:51.852851Z","iopub.execute_input":"2023-11-01T04:52:51.853210Z","iopub.status.idle":"2023-11-01T04:53:08.815334Z","shell.execute_reply.started":"2023-11-01T04:52:51.853184Z","shell.execute_reply":"2023-11-01T04:53:08.814428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Save Submission</b><a class='anchor' id='submission'></a> [↑](#top)\n\n***","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(paths.SUBMISSION_CSV)\nsubmission[\"generated\"] = predictions[\"predictions\"]\nsubmission[\"generated\"] = submission[\"generated\"].apply(lambda x: sigmoid(x))\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-11-01T04:53:08.817678Z","iopub.execute_input":"2023-11-01T04:53:08.818055Z","iopub.status.idle":"2023-11-01T04:53:08.833090Z","shell.execute_reply.started":"2023-11-01T04:53:08.818019Z","shell.execute_reply":"2023-11-01T04:53:08.832136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T04:53:08.834318Z","iopub.execute_input":"2023-11-01T04:53:08.834676Z","iopub.status.idle":"2023-11-01T04:53:08.841906Z","shell.execute_reply.started":"2023-11-01T04:53:08.834644Z","shell.execute_reply":"2023-11-01T04:53:08.840823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}